# Lab 7: Data cleaning & analysis {.unnumbered}

### Goals

- Read the data you need into R
- Select required variables
- Filter the data based on completeness (and any other criteria)
- Compute any required variables (scale means, number of items missing, etc)


## Data preparation

Building on the correlation example, we will include additional variables of interest - conscientiousness and agreeableness - to examine how these factors, along with extraversion, collectively predict feelings towards the Democratic party. Similar to the correlation project, we will start by cleaning and filtering the data, recoding the negatively-worded TIPI items (taking care to note which ones need recoding; it's not always the second question), and computing mean scores for each Big 5 trait.

Here is the pipeline to prepare the data:

```{r}
#| warning: false
#| message: false

library(tidyverse)
library(anesr)
data(timeseries_2016)

my_data_complete <- timeseries_2016 |> 
  select(extraversion1 = "V162333", 
         extraversion2 = "V162338",
         conscientiousness1 = "V162335", 
         conscientiousness2 = "V162340",
         agreeableness1 = "V162334", 
         agreeableness2 = "V162339",
         feeling_thermometer = "V161095") |> 
  filter(if_all(everything(), ~ . >= 0))  |>
  mutate(extraversion2 = 8 - extraversion2,
         conscientiousness2 = 8 - conscientiousness2,
         agreeableness1 = 8 - agreeableness1,
         extraversion_mean = rowMeans(across(contains("extraversion"))),
         conscientiousness_mean = rowMeans(across(contains("conscientiousness"))),
         agreeableness_mean = rowMeans(across(contains("agreeableness"))))

```


## Describing your variables

Just as in the previous lab, you'll need to compute the mean and standard deviation for each of your variables. Use the same process, replacing the variable names with your new ones:

```{r}
my_data_complete |>
  pivot_longer(everything(), 
               names_to = "variable", 
               values_to = "value", 
               values_transform = as.numeric) |> 
  group_by(variable) |> 
  summarize(count_valid = n(),
            mean = mean(value), 
            sd = sd(value))

```


## Visualizing the data

You can create histograms for each of your new variables, just like you did for extraversion. Since there are so many, rather than producing individual plots, I'm going to use the `pivot_longer()`, like we've done before grouping and summarizing several variables. In this case, however, we won't `summarize()`. Instead, the long-formatted data gets piped into `ggplot()`, and a `facet_wrap()` layer is added. That produces several sub-plots ("facets") based on the formula provided within the function. In this case, I want subplots for each different `variable` (the column with the names of the different variables), so the formula is just `~variable`. So the whole layer can be read as "facet wrap the plots by the different values of the "variable" column."

```{r}
#| include: false
theme_apa <- theme(
  panel.background = element_blank(),
  axis.line = element_line()
)
```

```{r}

my_data_complete |>
  select(extraversion1:agreeableness2) |> 
  pivot_longer(everything(), 
               names_to = "variable", 
               values_to = "value", 
               values_transform = as.numeric) |> 
  ggplot(aes(x = value)) +
  facet_wrap(~variable, nrow = 3) +
  geom_histogram(binwidth = 1, color = "white") +
  scale_x_continuous(breaks = 1:7) +
  theme_apa

```

Now I'll do the same for the 3 computed trait-means.

```{r}

my_data_complete |>
  select(contains("mean")) |> 
  pivot_longer(everything(), 
               names_to = "variable", 
               values_to = "value", 
               values_transform = as.numeric) |> 
  ggplot(aes(x = value)) +
  facet_wrap(~variable, nrow = 3) +
  geom_histogram(binwidth = 0.5, color = "white") +
  scale_x_continuous(breaks = 1:7) +
  theme_apa

```


## The regression analysis

Now we'll perform a multiple regression analysis. This allows us to examine the relationship between the single outcome variable (in this case, feeling_thermometer) and several independent variables (extraversion_mean, conscientiousness_mean, and agreeableness_mean).

```{r}
model <- lm(feeling_thermometer ~ extraversion_mean + conscientiousness_mean + agreeableness_mean, data = my_data_complete)

summary(model)
```


Under "Coefficients" you'll see an "Estimate" of the relationship between that predictor and the outcome variable, controlling for the other predictors. You'll also see a $t$-value and a $p$-value for each predictor, which tell you whether each predictor is significantly related to the outcome variable, controlling for the other predictors. The Estimate indicates how much the outcome variable changes on average with a one-unit increase in the predictor, holding all other predictors constant. So in this example, a one-point increase in conscientiousness predicts a $~1.6$ unit decrease on the feeling thermometer. A one-point increase in agreeableness predicts a $~1.2$ unit increase on the feeling thermometer. Both of these predictors are significantly associated with the feeling thermometer variable. Extraversion, however, is not a significant predictor; knowledge of somebody's extraversion score cannot reliably predict their feeling thermometer response.

The last part of the output gives the overall model fit. "Multiple R-squared" is the proportion of variance in the outcome variable that can be explained by the predictor variables, and Adjusted R-squared is a version of R-squared adjusted for the number of predictors.

Finally, the $F$-statistic and its corresponding $p$-value assess the overall statistical significance of the model. If the $p$-value is less than your desired significance level you can reject the null hypothesis that all the regression coefficients are zero.


