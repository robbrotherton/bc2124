# Lab 8: Analysis {.unnumbered}

### Goals

<!-- - Check the assumptions of multiple regression have been met -->
- Visualize relationships between predictors and outcome
- Compute your regression model
- Interpret the findings

<!-- ## Checking model assumptions -->

```{r}
#| include: false

library(tidyverse)
library(anesr)
data(timeseries_2016)

my_data_complete <- timeseries_2016 |> 
  select(extraversion1 = "V162333", 
         extraversion2 = "V162338",
         conscientiousness1 = "V162335", 
         conscientiousness2 = "V162340",
         agreeableness1 = "V162334", 
         agreeableness2 = "V162339",
         feeling_thermometer = "V161095") |> 
  filter(if_all(everything(), ~ . >= 0))  |>
  mutate(extraversion2 = 8 - extraversion2,
         conscientiousness2 = 8 - conscientiousness2,
         agreeableness1 = 8 - agreeableness1,
         extraversion_mean = rowMeans(across(contains("extraversion"))),
         conscientiousness_mean = rowMeans(across(contains("conscientiousness"))),
         agreeableness_mean = rowMeans(across(contains("agreeableness"))))

```


## Visualization

Previously, we visualized a bivariate correlation using a scatterplot. One way of visualizing a regression when the predictors are all measured on the same scale (as they are in this project) is to essentially produce several scatterplots overlaid on top of one another. The outcome variable will always be on the Y axis, and the values of the predictors on the X axis. Color (or some other aesthetic) is used to differentiate the different predictors.

This can be achieved by reshaping the data into long-format as we have done before. Here, however, as a parameter in `pivot_longer()` I specify `-feeling_thermometer`, meaning I want every column *except* the feeling thermometer to be pivoted longer. That keeps `feeling_thermometer` as a unique column of its own, duplicating its value across rows for each of the three predictor variables. The reshaped data can then be piped into `ggplot()`.

```{r}
#| include: false

theme_apa <- theme(
  panel.background = element_blank(),
  axis.line = element_line()
)
```

```{r}
my_data_complete |> 
  select(feeling_thermometer, contains("mean")) |> 
  pivot_longer(-feeling_thermometer) |> 
  ggplot(aes(x = value, y = feeling_thermometer, color = name)) +
  geom_point(position = position_jitter(width = 0.4, seed = 1), alpha = 0.3) +
  geom_smooth(method = "lm") + 
  theme_apa
```

This kind of visualization doesn't map on to the regression model perfectly; it essentially shows three bivariate correlations, while the regression model, in quantifying the relationship between each predictor and the outcome, also controls for the relationship between the predictor and the other predictors in the model. And with more than 3 or so predictors it can get visually messy and hard to easily interpret. For this project's design, however, it seems an appropriate choice.


## Analysis

### Computing regression in R

Now we'll compute the regression model. The model quantifies the relationship between the single outcome variable (in this case, `feeling_thermometer`) and several independent variables (`extraversion_mean`, `conscientiousness_mean`, and `agreeableness_mean`).

The `lm()` function compute a linear model. The first argument is a formula with the generic form `outcome ~ predictor_1 + predictor_2 + predictor_3 ...`, where those generic names will be replaced with the relevant column names from your data.frame. The second argument is `data`; the formula specifies the names of columns in a data.frame where the relevant values can be found, therefore the `data = ` argument is necessary to point R to the data.frame containing those columns.

By itself, the `lm()` function doesn't output all the information we need to report about the regression model. That's why I pipe the output of `lm()` into the `summary()` function, which outputs all the necessary information to the console.

```{r}

lm(feeling_thermometer ~ extraversion_mean + conscientiousness_mean + agreeableness_mean, 
   data = my_data_complete) |> 
  summary()

```


<!-- ### Checking model assumptions -->

```{r}
#| include: false
model <- glm(feeling_thermometer ~ extraversion_mean + conscientiousness_mean + agreeableness_mean, 
   data = my_data_complete)

performance::check_heteroscedasticity(model)
performance::check_collinearity(model)
performance::check_normality(model)
performance::check_outliers(model)
performance::check_autocorrelation(model)

# performance::check_model(model)
```

<!-- The assumptions of linearity, independence, homoscedasticity, multicollinearity, and normality were assessed and met. -->



### Interpreting the results

Under "Coefficients" you'll see an "Estimate" of the relationship between that predictor and the outcome variable, controlling for the other predictors. You'll also see a $t$-value and a $p$-value for each predictor, which tell you whether each predictor is significantly related to the outcome variable, controlling for the other predictors. The Estimate indicates how much the outcome variable changes on average with a one-unit increase in the predictor, holding all other predictors constant. So in this example, a one-point increase in conscientiousness predicts a $~1.6$ unit decrease on the feeling thermometer. A one-point increase in agreeableness predicts a $~1.2$ unit increase on the feeling thermometer. Both of these predictors are significantly associated with the feeling thermometer variable. Extraversion, however, is not a significant predictor; knowledge of somebody's extraversion score cannot reliably predict their feeling thermometer response.

The last part of the output gives the overall model fit. "Multiple R-squared" is the proportion of variance in the outcome variable that can be explained by the predictor variables, and Adjusted R-squared is a version of R-squared adjusted for the number of predictors.

Finally, the $F$-statistic and its corresponding $p$-value assess the overall statistical significance of the model. If the $p$-value is less than your desired significance level you can reject the null hypothesis that all the regression coefficients are zero.



